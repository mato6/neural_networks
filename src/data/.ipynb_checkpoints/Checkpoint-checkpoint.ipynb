{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install pandas\n",
    "#!pip install nltk\n",
    "#!pip install tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.width', 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "comments = pd.read_csv('../../data/attack_annotated_comments.tsv', sep='\\t')\n",
    "labels = pd.read_csv('../../data/attack_annotations.tsv', sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "comments = comments.drop(columns=['year','logged_in','ns','sample','split'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = labels.drop(columns=['worker_id','quoting_attack','recipient_attack','third_party_attack','other_attack'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "labels\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = labels.groupby('rev_id')['attack'].mean().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = comments.join(res.set_index('rev_id'), on='rev_id')\n",
    "data['attack'] = data['attack'].round(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.info()\n",
    "data.isnull().values.any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In out data are newlines, tabulators and quotions marks replaced, so we need to delete this replacement\n",
    "data['comment'] = data['comment'].apply(lambda x: x.replace(\"NEWLINE_TOKEN\",\"\"))\n",
    "data['comment'] = data['comment'].apply(lambda x: x.replace(\"TAB_TOKEN\",\"\"))\n",
    "data['comment'] = data['comment'].apply(lambda x: x.replace(\"``\",'\"'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Source: https://www.kaggle.com/prashantkikani/pooled-gru-with-preprocessing\n",
    "repl = {\n",
    "    \"&lt;3\": \" good \",\n",
    "    \":d\": \" good \",\n",
    "    \":dd\": \" good \",\n",
    "    \":p\": \" good \",\n",
    "    \"8)\": \" good \",\n",
    "    \":-)\": \" good \",\n",
    "    \":)\": \" good \",\n",
    "    \";)\": \" good \",\n",
    "    \"(-:\": \" good \",\n",
    "    \"(:\": \" good \",\n",
    "    \"yay!\": \" good \",\n",
    "    \"yay\": \" good \",\n",
    "    \"yaay\": \" good \",\n",
    "    \"yaaay\": \" good \",\n",
    "    \"yaaaay\": \" good \",\n",
    "    \"yaaaaay\": \" good \",\n",
    "    \":/\": \" bad \",\n",
    "    \":&gt;\": \" sad \",\n",
    "    \":')\": \" sad \",\n",
    "    \":-(\": \" bad \",\n",
    "    \":(\": \" bad \",\n",
    "    \":s\": \" bad \",\n",
    "    \":-s\": \" bad \",\n",
    "    \"&lt;3\": \" heart \",\n",
    "    \":d\": \" smile \",\n",
    "    \":p\": \" smile \",\n",
    "    \":dd\": \" smile \",\n",
    "    \"8)\": \" smile \",\n",
    "    \":-)\": \" smile \",\n",
    "    \":)\": \" smile \",\n",
    "    \";)\": \" smile \",\n",
    "    \"(-:\": \" smile \",\n",
    "    \"(:\": \" smile \",\n",
    "    \":/\": \" worry \",\n",
    "    \":&gt;\": \" angry \",\n",
    "    \":')\": \" sad \",\n",
    "    \":-(\": \" sad \",\n",
    "    \":(\": \" sad \",\n",
    "    \":s\": \" sad \",\n",
    "    \":-s\": \" sad \",\n",
    "    r\"\\br\\b\": \"are\",\n",
    "    r\"\\bu\\b\": \"you\",\n",
    "    r\"\\bhaha\\b\": \"ha\",\n",
    "    r\"\\bhahaha\\b\": \"ha\",\n",
    "    r\"\\bdon't\\b\": \"do not\",\n",
    "    r\"\\bdoesn't\\b\": \"does not\",\n",
    "    r\"\\bdidn't\\b\": \"did not\",\n",
    "    r\"\\bhasn't\\b\": \"has not\",\n",
    "    r\"\\bhaven't\\b\": \"have not\",\n",
    "    r\"\\bhadn't\\b\": \"had not\",\n",
    "    r\"\\bwon't\\b\": \"will not\",\n",
    "    r\"\\bwouldn't\\b\": \"would not\",\n",
    "    r\"\\bcan't\\b\": \"can not\",\n",
    "    r\"\\bcannot\\b\": \"can not\",\n",
    "    r\"\\bi'm\\b\": \"i am\",\n",
    "    \"m\": \"am\",\n",
    "    \"r\": \"are\",\n",
    "    \"u\": \"you\",\n",
    "    \"haha\": \"ha\",\n",
    "    \"hahaha\": \"ha\",\n",
    "    \"don't\": \"do not\",\n",
    "    \"doesn't\": \"does not\",\n",
    "    \"didn't\": \"did not\",\n",
    "    \"hasn't\": \"has not\",\n",
    "    \"haven't\": \"have not\",\n",
    "    \"hadn't\": \"had not\",\n",
    "    \"won't\": \"will not\",\n",
    "    \"wouldn't\": \"would not\",\n",
    "    \"can't\": \"can not\",\n",
    "    \"cannot\": \"can not\",\n",
    "    \"i'm\": \"i am\",\n",
    "    \"m\": \"am\",\n",
    "    \"i'll\" : \"i will\",\n",
    "    \"its\" : \"it is\",\n",
    "    \"'s\" : \" is\",\n",
    "    \"'s\" : \" is\",\n",
    "    \"`d\": \" would\",\n",
    "    \"that's\" : \"that is\",\n",
    "    \"weren't\" : \"were not\",\n",
    "    \"e.g\" : \"eg\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#splitting comments by spaces\n",
    "comments = data['comment'].apply(lambda x: x.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "abbr = [i for i in repl.keys()]\n",
    "result = []\n",
    "for index in range(len(comments)):\n",
    "    new_comment = \"\"\n",
    "    for word in comments[index]:\n",
    "        word = word.lower()\n",
    "        word = re.sub(r\"n't\", \" not\", word)\n",
    "        word = re.sub(r\"\\ 's\", \" not\", word)\n",
    "        word = re.sub(r\"\\ 've\", \" not\", word)\n",
    "        word = re.sub(r\"\\ 'd\", \" not\", word)\n",
    "        word = re.sub(r\"\\ ll\", \" not\", word)\n",
    "        if re.search(\"^http.+|^www.+\",word): # deleting links\n",
    "            continue\n",
    "        elif word in abbr:\n",
    "            new_comment += repl[word]\n",
    "            new_comment += \" \"\n",
    "            word = repl[word]\n",
    "        elif re.search(\"[^a-zA-Z ]+\",word): # only alphabet\n",
    "            new_comment += re.sub(\"[^a-zA-Z ]+\",\" \",word)\n",
    "            new_comment += \" \"\n",
    "        else:\n",
    "            new_comment += word  \n",
    "            new_comment += \" \"\n",
    "            \n",
    "    new_comment = \" \".join(new_comment.split())        \n",
    "   # print(\"Old = \", comments[index])\n",
    "   # print(\"New = \", new_comment)\n",
    "    result.append(new_comment)\n",
    "data[\"comment\"] = result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pd.options.display.max_rows = 999\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replacing empty strings with nan\n",
    "data['comment'].replace('', np.nan, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking nan values on data\n",
    "data.info()\n",
    "data.isnull().values.any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop nan rows\n",
    "data.dropna(subset=['comment'], inplace=True)\n",
    "data = data.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import SnowballStemmer\n",
    "\n",
    "def stemmer(comment):\n",
    "    stemmer = SnowballStemmer('english')\n",
    "    words = comment.split()\n",
    "    stemmed = [stemmer.stem(word) for word in words]\n",
    "    stemmed\n",
    "    return \" \".join(stemmed)\n",
    "\n",
    "t = data['comment'].map(lambda x: stemmer(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['stemmed_comments'] = t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving cleaned data\n",
    "data.to_csv('../../data/preprocessed_data.csv', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data= pd.read_csv('../../data/preprocessed_data.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13, 8, 11, 2516, 161, 17, 1, 1681, 437, 6, 1, 270, 3234, 5, 1934, 19, 565, 653, 2, 2134, 26, 4, 21, 11, 224, 9, 727, 1208, 397, 3, 42, 227, 83, 439, 402, 1761, 5, 11568, 3963, 5, 93, 10, 42, 15, 1137, 2, 224, 84, 1934, 5, 3234, 17, 209, 5, 84, 256, 209, 34, 2143, 1, 465, 4, 378, 8, 800, 417, 4, 54, 17, 11, 1310, 24, 1, 240, 561, 6, 1790, 173, 309, 556, 7220, 19, 1864, 12, 2673, 4219, 1284, 1, 3234, 610, 47, 1, 439, 402, 1761, 1479, 9, 1271, 1, 3224, 34, 15, 1875, 33, 1, 598, 26, 9, 8, 30, 2001, 727, 3, 43, 546, 9, 2, 290, 29, 6230, 3896, 254, 94, 27, 2516, 39, 10, 3, 318, 1859, 6, 724, 24, 4, 104, 1, 55, 22, 3025, 470, 711, 83, 1768, 5, 45, 8, 67, 52, 6, 2935, 105, 2935, 26, 3, 92, 2, 653, 23, 509, 3, 162, 15, 294, 2935, 2143, 2134, 2, 60, 14, 11, 1375, 2, 7, 5663, 3593, 20, 389, 39, 10, 8, 668, 5, 14, 1140, 1, 15135, 45, 8, 7, 1377, 509, 12, 23, 36, 26, 10, 189, 34, 68, 228, 200, 236, 47, 1313, 1074, 27, 318, 7, 162, 19, 888, 12, 4, 47, 10, 103, 26, 1, 437, 8, 588, 1059, 12, 5, 88, 6, 655, 226, 26, 10, 74, 63, 103, 354, 907, 27, 2613, 34, 1, 304, 2120, 12, 862, 2875, 409, 270, 4, 206, 10, 1500, 338, 417, 5146, 70, 778, 14, 31, 778, 538, 3225, 672, 4467, 5, 232, 44, 38, 429, 29, 4, 21, 11, 37, 21, 4, 71, 9, 5146, 87, 11, 619, 37, 1114, 8, 11, 417, 279, 52, 6, 270, 8, 1758, 67, 601, 417, 518, 3651, 5, 68, 5060, 2, 512, 13, 8, 23, 220, 11, 993, 4, 110, 2285, 1, 227, 727, 9, 43, 15, 546, 29, 2766, 1908, 7756, 6, 52850, 24, 99, 9, 5244, 12, 862, 966, 27, 632, 2, 74, 397, 1357, 47, 3234, 3437, 2143, 3437, 1934, 58, 17, 146, 12, 51, 378, 33, 31, 1060, 570, 327, 12, 31, 12, 795, 378, 8, 54, 7, 6574, 2, 1, 622, 26, 170, 189, 69, 2, 361, 7, 399, 47, 2053, 49, 98, 68, 795, 1258, 200, 26, 13, 8, 7, 200, 236, 727, 10, 20, 7, 200, 236, 29, 9, 25, 11, 37, 10, 25, 179, 2, 15]\n"
     ]
    }
   ],
   "source": [
    "tokenizer = Tokenizer()\n",
    "\n",
    "# Training of our tokenizer\n",
    "tokenizer.fit_on_texts(data['stemmed_comments'])\n",
    "#tokenizer.fit_on_texts(data['comment'])\n",
    "\n",
    "# Number of unique words in our data\n",
    "vocab_len = len(tokenizer.word_index) + 1\n",
    "\n",
    "#converting comment into numeric form, each unique word has a number, so comment will be rewrite into numbers\n",
    "embedded = tokenizer.texts_to_sequences(data['stemmed_comments'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "longest_sentence_len = len(max(embedded, key= len))\n",
    "\n",
    "#Adding zeroes to end of eat embeded sentece to len of the longest sentence\n",
    "padded_comments = pad_sequences(embedded, longest_sentence_len, padding='post')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Glove embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove = open('../../data/glove.6B.50d.txt', encoding=\"utf8\")\n",
    "from numpy import array\n",
    "from numpy import asarray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train and test data\n",
    "size_train = round(len(padded_comments)* 0.8)\n",
    "\n",
    "train_padded = padded_comments[:size_train]\n",
    "train_attack =asarray(data['attack'][:size_train], dtype='int32')\n",
    "\n",
    "test_padded = padded_comments[size_train + 1:]\n",
    "test_attack = asarray(data['attack'][size_train + 1:], dtype='int32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparing glove dictonary\n",
    "glove_dict = dict()\n",
    "\n",
    "#according to glove file, first is word and next are vectors\n",
    "for line in glove:\n",
    "    splitted = line.split()\n",
    "    word = splitted[0]\n",
    "    vectors = asarray(splitted[1:], dtype='float32')\n",
    "    glove_dict [word] = vectors \n",
    "\n",
    "\n",
    "glove.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import zeros\n",
    "zle = 0\n",
    "# create dict with only words in out data\n",
    "word_matrix = zeros((vocab_len, 50))\n",
    "# iterate throuh all words in our data and find vector for them\n",
    "for word, index in tokenizer.word_index.items():\n",
    "    vectors = glove_dict.get(word)\n",
    "    if vectors is not None:\n",
    "        word_matrix[index] = vectors # pretrained word embedings with words from our comments\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding,GRU,Dense\n",
    "from tensorflow.keras.initializers import Constant\n",
    "\n",
    "def GRU_model():\n",
    "    model = Sequential()\n",
    "    embedding_layer = Embedding(vocab_len,50,embeddings_initializer = Constant(word_matrix),input_length=longest_sentence_len,trainable=False)\n",
    "    model.add(embedding_layer)\n",
    "    model.add(GRU(units = 1, dropout=0.2, recurrent_dropout = 0.2))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, 2831, 50)          5603050   \n",
      "_________________________________________________________________\n",
      "gru (GRU)                    (None, 1)                 159       \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 1)                 2         \n",
      "=================================================================\n",
      "Total params: 5,603,211\n",
      "Trainable params: 161\n",
      "Non-trainable params: 5,603,050\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = GRU_model()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Not working in notebook and docker, trained in google colab, screen of results in model\n",
    "model.fit(train_padded,train_attack,batch_size=128, epochs=3, validation_data=(test_padded,test_attack),verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('GRU_model_1') "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
