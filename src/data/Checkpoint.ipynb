{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural networks, Checkpoint \n",
    "\n",
    "Faculty: FIIT <br>\n",
    "Academic year: 2019/2020 <br>\n",
    "Authors: Andrej Gáfrik, Martin Grega <br>\n",
    "\n",
    "\n",
    "We had to change out source data to https://figshare.com/articles/Wikipedia_Detox_Data/4054689, because of bad formatting in data, which we couldn't solve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting sklearn\n",
      "  Downloading https://files.pythonhosted.org/packages/1e/7a/dbb3be0ce9bd5c8b7e3d87328e79063f8b263b2b1bfa4774cb1147bfcd3f/sklearn-0.0.tar.gz\n",
      "Collecting scikit-learn (from sklearn)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/2e/d0/860c4f6a7027e00acff373d9f5327f4ae3ed5872234b3cbdd7bcb52e5eff/scikit_learn-0.22-cp36-cp36m-manylinux1_x86_64.whl (7.0MB)\n",
      "\u001b[K     |████████████████████████████████| 7.0MB 9.1MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting joblib>=0.11 (from scikit-learn->sklearn)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/8f/42/155696f85f344c066e17af287359c9786b436b1bf86029bb3411283274f3/joblib-0.14.0-py2.py3-none-any.whl (294kB)\n",
      "\u001b[K     |████████████████████████████████| 296kB 45.7MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.11.0 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->sklearn) (1.17.2)\n",
      "Requirement already satisfied: scipy>=0.17.0 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->sklearn) (1.3.3)\n",
      "Building wheels for collected packages: sklearn\n",
      "  Building wheel for sklearn (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for sklearn: filename=sklearn-0.0-py2.py3-none-any.whl size=2397 sha256=769d91b7b17a4c4a3775deb75baf2b09b1ef1b7c85f36afe2ff6de92151bf32d\n",
      "  Stored in directory: /root/.cache/pip/wheels/76/03/bb/589d421d27431bcd2c6da284d5f2286c8e3b2ea3cf1594c074\n",
      "Successfully built sklearn\n",
      "Installing collected packages: joblib, scikit-learn, sklearn\n",
      "Successfully installed joblib-0.14.0 scikit-learn-0.22 sklearn-0.0\n",
      "\u001b[33mWARNING: You are using pip version 19.2.3, however version 19.3.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# !pip install pandas\n",
    "# !pip install nltk\n",
    "# !pip install tokenizer\n",
    "#!pip uninstall keras\n",
    "#!pip install keras --upgrade\n",
    "!pip install sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.width', 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comments = pd.read_csv('../../data/attack_annotated_comments.tsv', sep='\\t')\n",
    "labels = pd.read_csv('../../data/attack_annotations.tsv', sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comments = comments.drop(columns=['year','logged_in','ns','sample','split'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "comments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Problems in data:\n",
    " - tabular, new line and qoutes were replaced with tab_token, new_line token and `\n",
    " - punctuation\n",
    " - more spaces between words\n",
    " - empty strings\n",
    " - abbreviations like i will = i'll , didn't = did not, ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = labels.drop(columns=['worker_id','quoting_attack','recipient_attack','third_party_attack','other_attack'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "labels\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = labels.groupby('rev_id')['attack'].mean().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = comments.join(res.set_index('rev_id'), on='rev_id')\n",
    "data['attack'] = data['attack'].round(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.info()\n",
    "data.isnull().values.any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In out data are newlines, tabulators and quotions marks replaced, so we need to delete this replacement\n",
    "data['comment'] = data['comment'].apply(lambda x: x.replace(\"NEWLINE_TOKEN\",\"\"))\n",
    "data['comment'] = data['comment'].apply(lambda x: x.replace(\"TAB_TOKEN\",\"\"))\n",
    "data['comment'] = data['comment'].apply(lambda x: x.replace(\"``\",'\"'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Source: https://www.kaggle.com/prashantkikani/pooled-gru-with-preprocessing\n",
    "repl = {\n",
    "    \"&lt;3\": \" good \",\n",
    "    \":d\": \" good \",\n",
    "    \":dd\": \" good \",\n",
    "    \":p\": \" good \",\n",
    "    \"8)\": \" good \",\n",
    "    \":-)\": \" good \",\n",
    "    \":)\": \" good \",\n",
    "    \";)\": \" good \",\n",
    "    \"(-:\": \" good \",\n",
    "    \"(:\": \" good \",\n",
    "    \"yay!\": \" good \",\n",
    "    \"yay\": \" good \",\n",
    "    \"yaay\": \" good \",\n",
    "    \"yaaay\": \" good \",\n",
    "    \"yaaaay\": \" good \",\n",
    "    \"yaaaaay\": \" good \",\n",
    "    \":/\": \" bad \",\n",
    "    \":&gt;\": \" sad \",\n",
    "    \":')\": \" sad \",\n",
    "    \":-(\": \" bad \",\n",
    "    \":(\": \" bad \",\n",
    "    \":s\": \" bad \",\n",
    "    \":-s\": \" bad \",\n",
    "    \"&lt;3\": \" heart \",\n",
    "    \":d\": \" smile \",\n",
    "    \":p\": \" smile \",\n",
    "    \":dd\": \" smile \",\n",
    "    \"8)\": \" smile \",\n",
    "    \":-)\": \" smile \",\n",
    "    \":)\": \" smile \",\n",
    "    \";)\": \" smile \",\n",
    "    \"(-:\": \" smile \",\n",
    "    \"(:\": \" smile \",\n",
    "    \":/\": \" worry \",\n",
    "    \":&gt;\": \" angry \",\n",
    "    \":')\": \" sad \",\n",
    "    \":-(\": \" sad \",\n",
    "    \":(\": \" sad \",\n",
    "    \":s\": \" sad \",\n",
    "    \":-s\": \" sad \",\n",
    "    r\"\\br\\b\": \"are\",\n",
    "    r\"\\bu\\b\": \"you\",\n",
    "    r\"\\bhaha\\b\": \"ha\",\n",
    "    r\"\\bhahaha\\b\": \"ha\",\n",
    "    r\"\\bdon't\\b\": \"do not\",\n",
    "    r\"\\bdoesn't\\b\": \"does not\",\n",
    "    r\"\\bdidn't\\b\": \"did not\",\n",
    "    r\"\\bhasn't\\b\": \"has not\",\n",
    "    r\"\\bhaven't\\b\": \"have not\",\n",
    "    r\"\\bhadn't\\b\": \"had not\",\n",
    "    r\"\\bwon't\\b\": \"will not\",\n",
    "    r\"\\bwouldn't\\b\": \"would not\",\n",
    "    r\"\\bcan't\\b\": \"can not\",\n",
    "    r\"\\bcannot\\b\": \"can not\",\n",
    "    r\"\\bi'm\\b\": \"i am\",\n",
    "    \"m\": \"am\",\n",
    "    \"r\": \"are\",\n",
    "    \"u\": \"you\",\n",
    "    \"haha\": \"ha\",\n",
    "    \"hahaha\": \"ha\",\n",
    "    \"don't\": \"do not\",\n",
    "    \"doesn't\": \"does not\",\n",
    "    \"didn't\": \"did not\",\n",
    "    \"hasn't\": \"has not\",\n",
    "    \"haven't\": \"have not\",\n",
    "    \"hadn't\": \"had not\",\n",
    "    \"won't\": \"will not\",\n",
    "    \"wouldn't\": \"would not\",\n",
    "    \"can't\": \"can not\",\n",
    "    \"cannot\": \"can not\",\n",
    "    \"i'm\": \"i am\",\n",
    "    \"m\": \"am\",\n",
    "    \"i'll\" : \"i will\",\n",
    "    \"its\" : \"it is\",\n",
    "    \"'s\" : \" is\",\n",
    "    \"'s\" : \" is\",\n",
    "    \"`d\": \" would\",\n",
    "    \"that's\" : \"that is\",\n",
    "    \"weren't\" : \"were not\",\n",
    "    \"e.g\" : \"eg\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#splitting comments by spaces\n",
    "comments = data['comment'].apply(lambda x: x.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "abbr = [i for i in repl.keys()]\n",
    "result = []\n",
    "for index in range(len(comments)):\n",
    "    new_comment = \"\"\n",
    "    for word in comments[index]:\n",
    "        word = word.lower()\n",
    "        word = re.sub(r\"n't\", \" not\", word)\n",
    "        word = re.sub(r\"\\ 's\", \" not\", word)\n",
    "        word = re.sub(r\"\\ 've\", \" not\", word)\n",
    "        word = re.sub(r\"\\ 'd\", \" not\", word)\n",
    "        word = re.sub(r\"\\ ll\", \" not\", word)\n",
    "        if re.search(\"^http.+|^www.+\",word): # deleting links\n",
    "            continue\n",
    "        elif word in abbr:\n",
    "            new_comment += repl[word]\n",
    "            new_comment += \" \"\n",
    "            word = repl[word]\n",
    "        elif re.search(\"[^a-zA-Z ]+\",word): # only alphabet\n",
    "            new_comment += re.sub(\"[^a-zA-Z ]+\",\" \",word)\n",
    "            new_comment += \" \"\n",
    "        else:\n",
    "            new_comment += word  \n",
    "            new_comment += \" \"\n",
    "            \n",
    "    new_comment = \" \".join(new_comment.split())        \n",
    "   # print(\"Old = \", comments[index])\n",
    "   # print(\"New = \", new_comment)\n",
    "    result.append(new_comment)\n",
    "data[\"comment\"] = result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pd.options.display.max_rows = 999\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replacing empty strings with nan\n",
    "data['comment'].replace('', np.nan, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking nan values on data\n",
    "data.info()\n",
    "data.isnull().values.any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop nan rows\n",
    "data.dropna(subset=['comment'], inplace=True)\n",
    "data = data.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import SnowballStemmer\n",
    "\n",
    "def stemmer(comment):\n",
    "    stemmer = SnowballStemmer('english')\n",
    "    words = comment.split()\n",
    "    stemmed = [stemmer.stem(word) for word in words]\n",
    "    stemmed\n",
    "    return \" \".join(stemmed)\n",
    "\n",
    "t = data['comment'].map(lambda x: stemmer(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['stemmed_comments'] = t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['number_of_words'] = data.stemmed_comments.apply(lambda x: len(x.split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.loc[data['number_of_words'] <= 300]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "rev_id                                                        6360046\n",
       "comment             wikipedia is an encyclopedia not a dictionary ...\n",
       "attack                                                              0\n",
       "stemmed_comments    wikipedia is an encyclopedia not a dictionari ...\n",
       "number_of_words                                                   300\n",
       "Name: 479, dtype: object"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.loc[data['number_of_words'].idxmax()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0    98887\n",
       "1.0    13206\n",
       "Name: attack, dtype: int64"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['attack'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving cleaned data\n",
    "data.to_csv('../../data/preprocessed_data_1.csv', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data= pd.read_csv('../../data/preprocessed_data_1.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer()\n",
    "\n",
    "# Training of our tokenizer\n",
    "tokenizer.fit_on_texts(data['stemmed_comments'])\n",
    "#tokenizer.fit_on_texts(data['comment'])\n",
    "\n",
    "# Number of unique words in our data\n",
    "vocab_len = len(tokenizer.word_index) + 1\n",
    "\n",
    "#converting comment into numeric form, each unique word has a number, so comment will be rewrite into numbers\n",
    "embedded = tokenizer.texts_to_sequences(data['stemmed_comments'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "longest_sentence_len = len(max(embedded, key= len))\n",
    "\n",
    "#Adding zeroes to end of eat embeded sentece to len of the longest sentence\n",
    "padded_comments = pad_sequences(embedded, longest_sentence_len, padding='post')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Glove embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove = open('../../data/glove.6B.100d.txt', encoding=\"utf8\")\n",
    "from numpy import array\n",
    "from numpy import asarray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train and test data\n",
    "size_train = round(len(padded_comments)* 0.8)\n",
    "\n",
    "train_padded = padded_comments[:size_train]\n",
    "train_attack =asarray(data['attack'][:size_train], dtype='int32')\n",
    "\n",
    "test_padded = padded_comments[size_train + 1:]\n",
    "test_attack = asarray(data['attack'][size_train + 1:], dtype='int32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparing glove dictonary\n",
    "glove_dict = dict()\n",
    "\n",
    "#according to glove file, first is word and next are vectors\n",
    "for line in glove:\n",
    "    splitted = line.split()\n",
    "    word = splitted[0]\n",
    "    vectors = asarray(splitted[1:], dtype='float32')\n",
    "    glove_dict [word] = vectors \n",
    "\n",
    "\n",
    "glove.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import zeros\n",
    "zle = 0\n",
    "# create dict with only words in out data\n",
    "word_matrix = zeros((vocab_len, 100))\n",
    "# iterate throuh all words in our data and find vector for them\n",
    "for word, index in tokenizer.word_index.items():\n",
    "    vectors = glove_dict.get(word)\n",
    "    if vectors is not None:\n",
    "        word_matrix[index] = vectors # pretrained word embedings with words from our comments\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GRU "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding,GRU,Dense\n",
    "from tensorflow.keras.initializers import Constant\n",
    "from sklearn.utils import class_weight\n",
    "\n",
    "def GRU_model():\n",
    "    model = Sequential()\n",
    "    embedding_layer = Embedding(vocab_len,100,embeddings_initializer = Constant(word_matrix),input_length=longest_sentence_len,trainable=False)\n",
    "    model.add(embedding_layer)\n",
    "    model.add(GRU(units = 64, dropout=0.2, recurrent_dropout = 0.2))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    model.compile(loss='mean_squared_error', optimizer='adam', metrics=['accuracy'])\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, 300, 100)          9365400   \n",
      "_________________________________________________________________\n",
      "gru (GRU)                    (None, 64)                31872     \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 9,397,337\n",
      "Trainable params: 31,937\n",
      "Non-trainable params: 9,365,400\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = GRU_model()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_weights = class_weight.compute_class_weight('balanced',\n",
    "                                                 np.unique(train_attack),\n",
    "                                                 train_attack)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0, 1], dtype=int32), array([78913, 10761]))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(train_attack, return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 0.5681826821943152, 1: 4.166620202583403}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights = {0: class_weights[0], 1: class_weights[1]}\n",
    "weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 89674 samples, validate on 22418 samples\n",
      "Epoch 1/3\n",
      "89674/89674 [==============================] - 307s 3ms/sample - loss: 0.3544 - accuracy: 0.6048 - val_loss: 0.2555 - val_accuracy: 0.1100\n",
      "Epoch 2/3\n",
      "89674/89674 [==============================] - 304s 3ms/sample - loss: 0.3930 - accuracy: 0.7314 - val_loss: 0.2550 - val_accuracy: 0.1100\n",
      "Epoch 3/3\n",
      "89674/89674 [==============================] - 303s 3ms/sample - loss: 0.4120 - accuracy: 0.7593 - val_loss: 0.2582 - val_accuracy: 0.1100\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f98d837fe80>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Not working in notebook and docker, trained in google colab, screen of results in model\n",
    "model.fit(train_padded,train_attack,batch_size=128, epochs=3, \n",
    "          validation_data=(test_padded,test_attack),verbose=1, class_weight=weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Results of training (because used wrong verbose parameter :( )\n",
    "- Epoch 1: 1110S - loss: 0.4846 - acc: 0.8798 - val_loss: 0.3513 - val_acc: 0.8951\n",
    "- Epoch 2: 1284S - loss: 0.3699 - acc: 0.8798 - val_loss: 0.3373 - val_acc: 0.8951\n",
    "- Epoch 3: 1290S - loss: 0.3681 - acc: 0.8798 - val_loss: 0.3571 - val_acc: 0.8951"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('GRU_model_1') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SimpleRNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding,Dense,SimpleRNN\n",
    "from tensorflow.keras.initializers import Constant\n",
    "\n",
    "def Simple_RNN():\n",
    "    model = Sequential()\n",
    "    embedding_layer = Embedding(vocab_len,100,embeddings_initializer = Constant(word_matrix),input_length=longest_sentence_len,trainable=False)\n",
    "    model.add(embedding_layer)\n",
    "    model.add(SimpleRNN(128,activation = 'tanh'))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    model.compile(loss='mean_squared_error', optimizer='adam', metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 300, 100)          9365400   \n",
      "_________________________________________________________________\n",
      "simple_rnn (SimpleRNN)       (None, 128)               29312     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 129       \n",
      "=================================================================\n",
      "Total params: 9,394,841\n",
      "Trainable params: 29,441\n",
      "Non-trainable params: 9,365,400\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model2 = Simple_RNN()\n",
    "model2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 89674 samples, validate on 22418 samples\n",
      "Epoch 1/3\n",
      "89674/89674 [==============================] - 139s 2ms/sample - loss: 0.2495 - accuracy: 0.4107 - val_loss: 0.2368 - val_accuracy: 0.8909\n",
      "Epoch 2/3\n",
      "89674/89674 [==============================] - 139s 2ms/sample - loss: 0.2468 - accuracy: 0.4029 - val_loss: 0.2415 - val_accuracy: 0.2359\n",
      "Epoch 3/3\n",
      "89674/89674 [==============================] - 138s 2ms/sample - loss: 0.2473 - accuracy: 0.4103 - val_loss: 0.2442 - val_accuracy: 0.2320\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f985850bcf8>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2.fit(train_padded,train_attack,batch_size=128, epochs=3, \n",
    "           validation_data=(test_padded,test_attack),verbose=1, class_weight=weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2.save('Simple_RNN_1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LSTM():\n",
    "    model = Sequential()\n",
    "    embedding_layer = Embedding(vocab_len,100,embeddings_initializer = Constant(word_matrix),input_length=longest_sentence_len,trainable=False)\n",
    "    model.add(embedding_layer)\n",
    "#     model.add(LSTM())\n",
    "#     model.add(GRU(64))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    model.compile(loss='mean_squared_error', optimizer='adam', metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model3 = LSTM()\n",
    "model3.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
